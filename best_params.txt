Best params:

Tuning:  Decision Tree
Fitting 3 folds for each of 960 candidates, totalling 2880 fits
Time tuning Decision Tree : 1.00m
Best params for Decision Tree : {'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'best'}

Tuning:  Naive Bayes
Fitting 3 folds for each of 100 candidates, totalling 300 fits
Time tuning Naive Bayes : 0.04623801310857137m
Best params for Naive Bayes : {'var_smoothing': np.float64(3.1992671377973845e-10)}

Tuning:  Random Forest
Fitting 3 folds for each of 25 candidates, totalling 75 fits
Time tuning Random Forest : 1.6
Best params for Random Forest : {'n_estimators': 165, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20, 'criterion': 'log_loss'}

Tuning:  Gradient Boosting
Fitting 3 folds for each of 25 candidates, totalling 75 fits
Time tuning Gradient Boosting : 0.2327301581700643m
Best params for Gradient Boosting : {'n_estimators': 32, 'min_samples_split': np.float64(0.7000000000000001), 'min_samples_leaf': np.float64(0.2), 'max_features': 4, 'max_depth': np.int64(10), 'learning_rate': 1}

Tuning:  AdaBoost
Fitting 3 folds for each of 25 candidates, totalling 75 fits
Time tuning AdaBoost : 1.2m
Best params for AdaBoost : {'n_estimators': 200, 'learning_rate': 1, 'algorithm': 'SAMME'}

Tuning:  XGBoost
Fitting 3 folds for each of 240 candidates, totalling 720 fits
Time tuning XGBoost : 6.9m
Best params for XGBoost : {'learning_rate': 0.1, 'max_depth': np.int64(10), 'n_estimators': 64}

Tuning:  K-Nearest Neighbor
Fitting 3 folds for each of 200 candidates, totalling 600 fits
Time tuning K-Nearest Neighbor : 0.9926558335622152s
Best params for K-Nearest Neighbor : {'leaf_size': 14, 'n_neighbors': np.int64(26), 'p': 1}

Tuning:  Logistic Regression
Fitting 3 folds for each of 10 candidates, totalling 30 fits
Time tuning Logistic Regression : 1.572838306427002s
{'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}

Tuning:  Stochastic Gradient Descent
Fitting 3 folds for each of 9 candidates, totalling 27 fits
Time tuning Stochastic Gradient Descent : 0.4570026397705078s
Best params for Stochastic Gradient Descent : {'l1_ratio': np.float64(0.75), 'penalty': 'elasticnet'}